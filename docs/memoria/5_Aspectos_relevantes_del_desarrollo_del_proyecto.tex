\capitulo{Aspectos relevantes del desarrollo del proyecto}
\label{cha:Aspectos relevantes del desarrollo del proyecto}

Este apartado contiene algunos aspectos relevantes sobre el desarrollo del
proyecto, incluyendo las razones detrás de las decisiones tomadas durante este y
el impacto que dichas decisiones han tenido sobre los resultados obtenidos.

\section{Fase de experimentación}

El conjunto de datos disponible durante la fase de experimentación consisten en
158 muestras con los siguientes ``atributos'':

\begin{itemize}
    \item Si la persona correspondiente a la instancia padece la enfermedad de
    Parkinson. Es lo que se busca predecir.
    \item Archivo de vídeo de una mano de la persona realizando la prueba de
    <<finger-tapping>>.
    \item Mano que aparece en el vídeo (izquierda o derecha).
    \item Fecha en la que se ha tomado el vídeo (irrelevante).
    \item Edad de la persona.
    \item Sexo de la persona.
    \item Mano dominante de la persona.
\end{itemize}

Cabe destacar que se ha tomado una muestra de cada mano de cada persona. Por lo
que este conjunto de datos ha sido obtenido de 79 personas. Aún así, se ha
tomado cada mano como una instancia distinta con el objetivo de tener más
instancias con las que trabajar.

Además, se dispone de 69 instancias de personas que padecen la enfermedad de
Parkinson y 89 instancias de personas sanas, por lo que se está trabajando con
un conjunto de datos ligeramente desbalanceado. Esto deberá tenerse en cuenta
para que no influya en los resultados obtenidos, por ejemplo, seleccionando
métricas adecuadas.

\subsection{Procesado de vídeo}

El objetivo de la fase de experimentación es crear un sistema que tenga por
entrada los atributos antes mencionados (salvo el objetivo) y por salida si esos
atributos corresponden con una persona que padece la enfermedad de Parkinson
(posiblemente junto con el grado de confianza de la predicción).

Los algoritmos de aprendizaje automático típicamente están diseñados para
trabajar con atributos numéricos, por lo que los atributos disponibles deben ser
transformados durante el preprocesado. Esto se puede realizar en la mayoría de
casos mediante simples métodos de minería de datos, como, por ejemplo, la
codificación de variables categóricas. Pero hay un problema, el archivo de
vídeo.

En este caso, una imagen es una matriz bidimensional donde cada celda contiene
una 3-tupla representando los valores rojo, verde y azul del pixel con el que se
corresponde. Un vídeo es una secuencia de imágenes junto con información
adicional, es este caso sólamente es relevante la tasa de refresco, que es la
frecuencia a la que ha sido capturada cada imagen. Esto es de gran importancia
para dar una componente temporal al vídeo.

Con lo anterior se puede ver que un vídeo no se puede utilizar de forma directa,
sino que deberá pasar por una fase multietapa de preprocesado para ser
transformado a un conjunto de valores numéricos que lo describen.

\subsubsection{Extracción de <<landmarks>>}

Una <<landmark>> en el campo de la visión aritificial es un punto de referencia
que se corresponde con un objeto físico. El objetivo de esta fase es convertir
el vídeo de la prueba de <<finger-tapping>> a una secuencia o serie temporal de
conjuntos de <<landmarks>> los cuales representan la posición de la mano durante
un instante concreto.

El primer paso para lograr esto es leer las imágenes del archivo de vídeo, esto
ha sido logrado mediante la librería de <<bindings>> de OpenCV para Python.
OpenCV es una librería de uso general para realizar tareas relacionadas con la
visión artificial, y permite leer multitud de formatos de vídeo mediante un
flujo de imágenes.

Se ha tomado especial precaución para no cargar en memoria todas las imágenes
del vídeo, sino ir leyéndolas cuando son necesarias. Un archivo de vídeo ocupa
relativamente poco espacio en disco debido a que está codificado mediante un
<<codec>> determinado, lo que reduce el tamaño del archivo. Pero, para poder
trabajar con las imágenes del vídeo, este debe ser decodificado.

La magnitud de lo anterior se ve claramente con el siguiente ejemplo. Un vídeo
con una resolución de 1920x1080 a 30 fotogramas por segundo codificado mediante
el codec H264 que ocupa 50,7MB, decodificado, ocuparía en torno a los 3,7GB.

A continuación se deben extraer las <<landmarks>> de las imágenes leidas. Esto
ha sido logrado mediante la librería Mediapipe Hands \cite{zhang2020mediapipe},
que permite extraer los siguentes puntos a partir de una imágen:

\imagen{relevant_aspects/hand-landmarks.png}{Mediapipe Hands <<Landmarks>>}{1}

Debido a la perspectiva desde la que se han tomados los vídeos de la prueba de
<<finger-tapping>> hay algunas <<landmarks>> que quedan ocultas en varias
ocasiones, lo que hace su posición detectada poco fiable. Por lo que se desechan
las <<landmarks>> en el intervalo [9-17] para solo tomar en cuenta la parte
``frontal'' de la mano.

Mediapipe Hands ofrece la posibilidad de extraer <<landmarks>> de las imágenes
como si estas no tuviesen relación entre sí o realizar la extracción teniendo en
cuenta que se trata de un vídeo, así teniendo en cuenta resultados anteriores
para determinar de forma más precisa las <<landmarks>> del fotograma actual.
Esto tiene la gran desventaja de que hace imposible la paralelización de este
proceso (debido a que cada extracción depende de los resultados anteriores).

Lo anterior no tiene un gran efecto durante la fase de investigación, ya que sí
se puede paralelizar la extracción de <<landmarks>> de cada vídeo. Donde sí
tiene efecto es sobre la aplicación web, empeorando la experiencia de los
usuarios.

En última instancia se tomó la decisión de no usar paralelización, y así obtener
resultados más precisos. Una alternativa planteada fue realizar la extracción en
paralelo solamente en la aplicación, pero esto se descartó ya que esto podría
introducir inconsistencias y variables adicionales que deberían ser tomadas en
cuenta.

\subsubsection{Extracción de series temporales}

Tras el proceso descrito en el apartado anterior se dispone una secuencia de
datos similar a la siguiente:

\imagen{relevant_aspects/poses_series.png}{Serie temporal de <<landmarks>>}{1}

Cada fila representa la pose de la mano durante un fotograma concreto. Se puede
observar que el índice es una fecha completa en lugar de otras opciones más
convenientes como los segundos desde el inicio del vídeo o lo que se conoce como
<<timestamp>>. Esto es una imposición de TSFresh para poder realizar la
extracción de características teniendo en cuenta esta componente temporal.

Cada celda representa el punto en el espacio en el que se encuentra la
<<landmark>> correspondiente en un instante de tiempo.

Aunque estas series temporales son una simplificación muy grande de lo vídeos
iniciales, aún no son del todo útiles. Al ser información sobre la posición en
el espacio, es dependiente de la forma en la que ha sido tomada la grabación. En
un entorno controlado esto no sería mucho problema, pero en este caso, como se
desea hacer el uso del sistema accesible a cualquier persona sin necesitar
equipamiento especial, se deben buscar alternativas.

En este caso se ha decidido tomar el ángulo entre el ángulo entre la punta del
dedo pulgar, la muñeca y la punta del dedo índice. Esta medida es útil ya que no
depende de cómo se ha tomado el vídeo ni del tamaño de la mano de la persona.
Quedando como resultado una serie temporal como la siguiente:

\imagen{relevant_aspects/angle_series.png}{Serie temporal de ángulos}{0.5}

La unidad usada en estos ángulos es el radián.

\subsubsection{Extracción de características}

El último paso para convertir un archivo de vídeo a atributos útiles para
entrenar un modelo de aprendizaje es extraer características que describan la
serie temporal obtenida. Algunos de estos atributos han sido extraídos de forma
específica teniendo en cuenta este problema concreto, mientras que otros (la
mayoría) son atributos genéricos de series temporales.

\paragraph{Detección de toques y amplitudes}

Para varios atributos específicos al problema han sido utilizados los instantes
de tiempo en los que se producen toques (el pulgar y el índice tocan) y
amplitudes máximas en el movimiento de <<finger-tapping>>.

La detección de estos instantes ha sido realizada mediante una búsqueda de
máximos gracias a la función \texttt{find\_peaks} de la librería
\href{https://scipy.org/}{scipy}. Pero esta función por si sola no da muy buenos
resultados.

\imagen{relevant_aspects/raw_peaks.png}{Amplitudes}{0.7}

El eje horizontal representa el tiempo que ha transcurrido desde el inicio del
vídeo. El eje vertical representa el ángulo en radianes entre el dedo índice, la
muñeca y el pulgar.

Se puede apreciar que obtener todos los máximos locales no es la mejor opción,
debe incorporarse algún criterio para determinar si un máximo debería tenerse en
cuenta o no. La función \texttt{find\_peaks} tiene algunos parámetros para este
propósito.

Tras probar varias opciones y comprobar manualmente los resultados obtenidos
sobre diferentes vídeos, se determinó que la mejor opción es establecer la
prominencia mínima (parámetro \texttt{prominence} de find\_peaks) que debe tener
un máximo local para ser considerado.

La prominencia se define como la medida que determina cuánto destaca un máximo
con respecto a sus máximos más cercanos. Se probaron varios valores estáticos,
pero como cada vídeo tiene características diferentes era necesario utilizar
algo más dinámico. La desviación típica se comportaba bastante bien, pero cuando
la amplitud variaba con el tiempo (síntoma de la enfermedad de Parkinson) los
máximos de menor magnitud no eran detectados.

Lo anterior fue solucionado  mediante el uso de una desviación típica móvil. Se
establece una ventana que determina la ``zona'' a la que pertenece un valor y se
utiliza la desviación típica de esa ``zona'' como prominencia mínima para que el
punto pueda ser considerado un máximo.

\imagen{relevant_aspects/rolling_std_peaks.png}{Amplitudes mediante desviación típica móvil}{0.7}

Se puede apreciar que los resultados son ampliamente mejores. Incluso cuando la
amplitud decrece con el tiempo:

\imagen{relevant_aspects/rolling_std_peaks_id.png}{Amplitudes mediante desviación típica móvil con la enfermedad de Parkinson}{0.7}

Aunque si el movimiento es errático, los resultados también lo son, como es de
esperar.

\imagen{relevant_aspects/rolling_std_peaks_erratic.png}{Amplitudes mediante desviación típica móvil con movimiento errático}{0.7}

Para detectar los toques (mínimos) es tan simple como utilizar la serie de
ángulos negada para detectar los puntos donde se encuentran los máximos. Además
se ha establecido un umbral para que solo se consideren toques si se encuentra
entre 0 y 0.1 radianes.

\imagen{relevant_aspects/rolling_std_valleys.png}{Amplitudes mediante desviación típica móvil con movimiento errático}{0.7}

\paragraph{Características detectadas}

A continuación se van a detallar algunas de las características extraídas de
mayor interés. Estos valores ya son atributos que describen el vídeo, por lo que
pueden ser utilizados para la generación de un modelo.

\begin{itemize}
    \item \textbf{Velocidad media del movimiento}: Una característica de la
    bradicinesia es la ralentización del movimiento. Esto va a ser reflejado en
    la velocidad media (en radianes por segundo) que tiene el movimiento
    \item \textbf{Frecuencia de amplitudes y toques}: Dos magnitudes que miden
    cuantos toques o amplitudes se realizan por segundo de media durante el
    movimineto.
    \item \textbf{Amplitud media}: Otra característica de la enfermedad de
    Párkinson es la debilidad, lo que se puede ver reflejado sobre la amplitud
    media del movimiento. Es simplemente la media de las amplitudes antes
    detectadas.
\end{itemize}

Además de las anteriores, se extrayeron 794 características genéricas
mediante la librería TSFresh.

\subsection{Limpieza de datos}

En este punto se dispone de un conjunto de 158 instancias y unos 800 atributos,
algunos de ellos categóricos, estos deberán ser codificados para que sean
atributos numéricos, con los que se trabaja mejor.

Además, se determinó que todas las instancias deberán tener un tiempo de
detección de la mano realizando la prueba de <<finger-tapping>> mínimo de 15
segundos para garantizar en cierto modo la precisión de los atributos extraidos
y poder compara instancias con el mínimo número de influencias sobre las que no
se tiene control.

Debido a la decisión anterior se pasó de disponer de 158 instancias a únicamente
156. Esto es desafortunado debido al ya pequeño número de muestras, pero se
consideró una pérdida que merece la pena con el fin de obtener mejores
resultados.

Por último se sustituyeron las característica <<mano dominante>> y <<mano que
aparece en el vídeo>> por una única característica <<es la mano que aparece en
el vídeo la dominante>> con el objetivo de reducir la conocida ``maldición de la
dimensionalidad'' al mismo tiempo que se mantiene la información relevante.
